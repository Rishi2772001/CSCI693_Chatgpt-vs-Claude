# CSCI693_Chatgpt-vs-Claude

Measureâ€”**quickly and repeatably**â€”how well ChatGPT and Claude solve the same programming problems.

| ðŸ”Ž Metric              | âœ… Tracked |
|------------------------|-----------|
| Unit-test correctness  | âœ” |
| Wall-clock runtime     | âœ” |
| Peak memory usage      | âœ” |
| Line coverage (%)      | âœ” |

The repo ships with a growing **dataset of coding tasks** (`datasets/`) arranged by difficulty.  
For every task you get:

* the original natural-language prompt  
* each modelâ€™s generated solution (`chatgpt_solution.py`, `claude_solution.py`)  
* a small test-suite (`testcases.py`)  
* an **out-of-the-box harness** (`compare_solutions.py`) that logs the metrics above to `final_comparison.csv`.

---

## 1 Â· Project goals

1. **Reproducible evaluation** â€“ anyone can re-run every benchmark locally.  
2. **Fine-grained insight** â€“ see where one model is faster, leaner, or more accurate.  
3. **Extensibility** â€“ plug new problems, new models, or new metrics in minutes.

---

## 2 Â· Quick start

```bash
# âžŠ Clone the repo
git clone https://github.com/Rishi2772001/CSCI693_Chatgpt-vs-Claude.git
cd CSCI693_Chatgpt-vs-Claude
```
### Run a single benchmark 
```bash
cd datasets/EASY_PROBLEMS/problem\ 11
python compare_solutions.py
```

